{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "print(spark.version)\n",
        "print(spark.sparkContext.master)\n",
        "spark.range(100000).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "rdd.map(lambda x: x * 2).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "df = spark.createDataFrame([(1,), (2,), (3,)], [\"value\"])\n",
        "df.selectExpr(\"value * 2 as value\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "df = spark.range(1_000_000)\n",
        "df2 = df.filter(\"id % 2 = 0\")\n",
        "print(\"Spark ещё ничего не считал\")\n",
        "\n",
        "df2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql import functions as F\n",
        "data = [\n",
        "    (\"user1\",\"A\",100),\n",
        "    (\"user1\",\"B\",200),\n",
        "    (\"user2\",\"A\",50),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"user\",\"category\",\"amount\"])\n",
        "df.groupBy(\"user\").agg(F.sum(\"amount\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql.functions import broadcast\n",
        "users = spark.createDataFrame(\n",
        "    [(\"user1\",\"Berlin\"),(\"user2\",\"Munich\")],\n",
        "    [\"user\",\"city\"]\n",
        ")\n",
        "orders = spark.createDataFrame(\n",
        "    [(\"user1\",100),(\"user2\",300)],\n",
        "    [\"user\",\"amount\"]\n",
        ")\n",
        "users.join(orders, \"user\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "broadcast(users).join(orders, \"user\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, desc\n",
        "df = spark.createDataFrame(\n",
        "    [(\"user1\",100),(\"user1\",200),(\"user1\",50)],\n",
        "    [\"user\",\"amount\"]\n",
        ")\n",
        "w = Window.partitionBy(\"user\").orderBy(desc(\"amount\"))\n",
        "df.withColumn(\"rn\", row_number().over(w)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql import functions as F\n",
        "big = spark.range(5_000_000).withColumn(\"x\", (F.col(\"id\") % 10).cast(\"int\"))\n",
        "big.cache()\n",
        "big.count()\n",
        "big.filter(\"x == 1\").count()\n",
        "big.filter(\"x == 2\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "path = \"s3a://hadoop/demo/\"\n",
        "df = spark.range(10000)\n",
        "df.write.mode(\"overwrite\").parquet(path)\n",
        "spark.read.parquet(path).count()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "python",
      "pygments_lexer": "scala",
      "version": "3.13.9"
    },
    "name": "Revina"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
