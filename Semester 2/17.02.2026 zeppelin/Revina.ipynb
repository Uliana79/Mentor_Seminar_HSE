{
  "metadata": {
    "name": "Revina",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nprint(spark.version)\r\nprint(spark.sparkContext.master)\r\nspark.range(100000).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrdd \u003d sc.parallelize([1,2,3,4,5])\nrdd.map(lambda x: x * 2).collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d spark.createDataFrame([(1,), (2,), (3,)], [\"value\"])\ndf.selectExpr(\"value * 2 as value\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d spark.range(1_000_000)\ndf2 \u003d df.filter(\"id % 2 \u003d 0\")\nprint(\"Spark ещё ничего не считал\")\n\ndf2.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import functions as F\ndata \u003d [\n    (\"user1\",\"A\",100),\n    (\"user1\",\"B\",200),\n    (\"user2\",\"A\",50),\n]\ndf \u003d spark.createDataFrame(data, [\"user\",\"category\",\"amount\"])\ndf.groupBy(\"user\").agg(F.sum(\"amount\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import broadcast\nusers \u003d spark.createDataFrame(\n    [(\"user1\",\"Berlin\"),(\"user2\",\"Munich\")],\n    [\"user\",\"city\"]\n)\norders \u003d spark.createDataFrame(\n    [(\"user1\",100),(\"user2\",300)],\n    [\"user\",\"amount\"]\n)\nusers.join(orders, \"user\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbroadcast(users).join(orders, \"user\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, desc\ndf \u003d spark.createDataFrame(\n    [(\"user1\",100),(\"user1\",200),(\"user1\",50)],\n    [\"user\",\"amount\"]\n)\nw \u003d Window.partitionBy(\"user\").orderBy(desc(\"amount\"))\ndf.withColumn(\"rn\", row_number().over(w)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import functions as F\nbig \u003d spark.range(5_000_000).withColumn(\"x\", (F.col(\"id\") % 10).cast(\"int\"))\nbig.cache()\nbig.count()\nbig.filter(\"x \u003d\u003d 1\").count()\nbig.filter(\"x \u003d\u003d 2\").count()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npath \u003d \"s3a://hadoop/demo/\"\ndf \u003d spark.range(10000)\ndf.write.mode(\"overwrite\").parquet(path)\nspark.read.parquet(path).count()"
    }
  ]
}